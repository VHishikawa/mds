from confluent_kafka import Consumer
from time import sleep

broker = "broker:9092"
topic = "t_teste"
group_id = "consumer-2"

consumer_config = {
    'bootstrap.servers': broker,
    'group.id': group_id,
    'auto.offset.reset': 'earliest',
    'enable.auto.commit': 'false',
    'max.poll.interval.ms': '86400000'
}

consumer = Consumer(consumer_config)
consumer.subscribe([topic])     


try:
    while True:
        print("Listening")
        # read single message at a time
        msg = consumer.poll(0)
        
        if msg is None:
            sleep(5)
            continue
        if msg.error():
            print("Error reading message : {}".format(msg.error()))
            continue
        # You can parse message and save to data base here
        print(msg.offset())
        print(msg.topic())
        print(msg.value())
        
        consumer.commit()

except Exception as ex:
    print("Kafka Exception : {}", ex)

finally:
    print("closing consumer")
    consumer.close()

#RUNNING CONSUMER FOR READING MESSAGE FROM THE KAFKA TOPIC
#my_consumer = ExampleConsumer()
#my_consumer.start_listener()



---
from confluent_kafka import Producer
import json
p=Producer({'bootstrap.servers':'broker:9092'})
m=json.dumps('{"id":1,"nome":"fabio"}')
p.poll(1)
        
p.produce('t_teste', m.encode('utf-8'))
p.flush()


---
kafka-clients-3.3.1.jar
spark-sql-kafka-0-10_2.12-3.3.1.jar
spark-streaming-kafka-0-10_2.12-3.3.1.jar
spark-token-provider-kafka-0-10_2.12-3.3.1.jar
delta-core_2.12-2.2.0.jar
delta-storage-2.2.0.jar
pip install delta-spark

hadoop-aws-3.3.1.jar
aws-java-sdk-1.12.392.jar
aws-java-sdk-core-1.12.392.jar
aws-java-sdk-sts-1.12.392.jar
aws-java-sdk-s3-1.12.392.jar
aws-java-sdk-dynamodb-1.12.392.jar



--packages io.delta:delta-core_2.12:2.2.0,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.392 \

 wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.2.jar
 wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.392/aws-java-sdk-bundle-1.12.392.jar

rm hadoop-aws-3.3.1.jar aws-java-sdk-1.12.392.jar aws-java-sdk-core-1.12.392.jar aws-java-sdk-sts-1.12.392.jar aws-java-sdk-s3-1.12.392.jar aws-java-sdk-dynamodb-1.12.392.jar



CALL delta.system.register_table(schema_name => 'myschema', table_name => 'results', table_location => 's3a://delta-lake/demo2');

https://api.publicapis.org/entries

https://www.boredapi.com/api/activity/
https://random-data-api.com/api/v2/users
https://ciprand.p3p.repl.co/api?len=20&count=1
https://catfact.ninja/fact
https://ciprand.p3p.repl.co/api?len=20&count=1
https://api.publicapis.org/entries
https://randomuser.me/api/


https://mtpatter.github.io/bilao/notebooks/html/01-spark-struct-stream-kafka.html

{ "dt":"2023-01-31 20:44:43","activity":"Explore the nightlife of your city","type":"social","participants":1,"price":0.1,"link":"","key":"2237769","accessibility":0.32}'


confluent-hub install debezium/debezium-connector-postgresql:latest

curl -sS localhost:8083/connector-plugins

https://docs.confluent.io/kafka-connectors/debezium-postgres-source/current/overview.html#supports-one-task

curl -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @register-connector.json

curl -i -X PUT -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/inventory-connector/config -d '{ "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "debezium", "database.password": "dbz", "database.server.id": "184054", "database.server.name": "dbserver1", "database.include.list": "inventory", "database.history.kafka.bootstrap.servers": "kafka:9092", "database.history.kafka.topic": "dbhistory.inventory" }'

 curl -i -X DELETE localhost:8083/connectors/dvdrental

curl -i -X PUT -H "Accept:application/json" -H "Content-Type:application/json" localhost:8083/connectors/inventory-connector/config -d '{ "connector.class": "io.debezium.connector.mysql.MySqlConnector", "tasks.max": "1", "database.hostname": "mysql", "database.port": "3306", "database.user": "debezium", "database.password": "dbz", "database.server.id": "184054", "database.server.name": "dbserver1", "database.include.list": "inventory", "database.history.kafka.bootstrap.servers": "kafka:9092", "database.history.kafka.topic": "dbhistory.inventory" }'

curl.exe -i -X POST -H "Accept:application/json" -H  "Content-Type:application/json" http://localhost:8083/connectors/ -d @rest-connector.json

{
  "name": "source_rest_api",
  "config": {
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "org.apache.kafka.connect.storage.StringConverter",
    "connector.class": "com.tm.kafka.connect.rest.RestSourceConnector",
    "tasks.max": "1",
    "rest.source.poll.interval.ms": "30000",
    "rest.source.method": "GET",
    "rest.source.url": "https://random-data-api.com/api/v2/users",
    "rest.source.headers": "Content-Type:application/json,Accept:application/json",
    "rest.source.topic.selector": "com.tm.kafka.connect.rest.selector.SimpleTopicSelector",
    "rest.source.destination.topics": "cliente"
  }
}


--show TOPICS;

--show STREAMS;

CREATE STREAM city (city_id int, city string,counntry_id int, last_update string) WITH (kafka_topic='postgres.public.city', value_format='json', partitions=1);

https://hub.docker.com/r/apache/spark-py


https://rmoff.net/2020/07/03/why-json-isnt-the-same-as-json-schema-in-kafka-connect-converters-and-ksqldb-viewing-kafka-messages-bytes-as-hex/



pinot+http://pinot-broker:8099/query?controller=http://pinot-controller:9000/``
